# This YAML file is an example of a simple language model which can be trained
# on the Penn Treebank data. It uses a projection layer to embed words in a
# 128-dimensional space. The concatenated embeddings go through a hidden tanh
# layer after which a softmax output layer gives the word probabilities.
#
# This model trains for 72 epochs (approx. 1.5 hour) and achieves a
# perplexity of 194.16 on the validation set (nll: 5.09).

!obj:pylearn2.train.Train {
  dataset: &train !obj:pylearn2.sandbox.nlp.datasets.penntree.PennTreebank_Sequences {
      which_set: 'train',
      context_len: &context_len 1,
      data_mode: &data_mode 'chars'
  },
  model: !obj:pylearn2.models.mlp.MLP {
    input_space: !obj:pylearn2.sandbox.rnn.space.SequenceSpace {
      space: !obj:pylearn2.space.IndexSpace {
        dim: 1,
        max_labels: 51,
      },
    },
    layers: [
      !obj:pylearn2.sandbox.nlp.models.mlp.ProjectionLayer {
         layer_name: 'projection',
         dim: 300,
         irange: 0.01
      },
      !obj:pylearn2.sandbox.rnn.models.mlp.Recurrent {
         layer_name: 'recurrent_layer',
         dim: 500,
         irange: 0.01,
      },
      !obj:pylearn2.sandbox.nlp.models.mlp.Softmax {
         n_classes: 51,
         layer_name: 'softmax',
         irange: 0.01
      }
    ],
  },
  algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
    batch_size: 256,
    learning_rate: .000001,
    learning_rule: !obj:pylearn2.training_algorithms.learning_rule.AdaDelta {},
    monitoring_dataset: {
      'valid': !obj:pylearn2.sandbox.nlp.datasets.penntree.PennTreebank_Sequences {
        which_set: 'valid',
        context_len: *context_len,
        data_mode: *data_mode
      }
    },
  },
  save_path: '/Tmp/devincol/char_language_model.pkl',
  save_freq: 1
}